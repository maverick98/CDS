{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/OZow1VxwbgKELiJdQkeI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maverick98/CDS/blob/main/image_caption_transformer_github.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import necessary libraries"
      ],
      "metadata": {
        "id": "y3PVm73kLwfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
        "from tensorflow.data import Dataset\n",
        "from tensorflow.keras.layers import Embedding,Dense, Layer,TextVectorization\n",
        "from tensorflow.keras.backend import softmax\n",
        "from pickle import load, dump, HIGHEST_PROTOCOL\n",
        "from sklearn.utils import shuffle\n",
        "from numpy import savetxt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow import convert_to_tensor, int64\n",
        "from tensorflow.keras.layers import LayerNormalization,   ReLU, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "from tensorflow.keras.metrics import Mean\n",
        "from tensorflow import data, train,  reduce_sum,  equal, argmax,GradientTape, function\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from time import time\n",
        "from pickle import dump\n",
        "from tensorflow import    linalg, ones, maximum, newaxis\n",
        "from tensorflow.keras import Model\n",
        "from pickle import load\n",
        "from tensorflow import Module\n",
        "from tensorflow import   TensorArray, argmax,  transpose\n",
        "from matplotlib.pylab import plt\n",
        "from numpy import arange\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from glob import glob\n",
        "import PIL\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "#import gensim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from PIL import Image\n",
        "import random\n",
        "from importlib.machinery import SourceFileLoader\n",
        "from os.path import join\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "n6BDkBbELu8q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ],
      "metadata": {
        "id": "XM1nAEvvKm1E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixOYqR2r7T-Q",
        "outputId": "728a174f-45ad-4fc7-c59c-a8f801ff3825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "ROOT = \"/content/drive\"\n",
        "drive.mount(ROOT)\n",
        "PROJ = \"MyDrive/Capstone/src\" \n",
        "PROJECT_PATH = join(ROOT, PROJ)\n",
        "!mkdir -p \"{PROJECT_PATH}\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BGSNYEmfHsgv",
        "outputId": "3f2289ec-615e-444d-827f-6d63efaa928c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Capstone/src'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check if connected to GPU"
      ],
      "metadata": {
        "id": "KujElPN-Orji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_5u_9YyOpNz",
        "outputId": "009c159d-39af-49d0-fcb1-35cc6dc9d4a4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not connected to a GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Modules"
      ],
      "metadata": {
        "id": "X2WOLpG1LoqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_module(module_name):\n",
        "    module_py=module_name+'.py'\n",
        "    SourceFileLoader(module_name, join(PROJECT_PATH, module_py)).load_module()\n",
        "    \n"
      ],
      "metadata": {
        "id": "ZjQo8Ho19CS1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_module('transformer_model_params')\n",
        "load_module('transformer_training_params')\n",
        "load_module('positional_encoding')"
      ],
      "metadata": {
        "id": "5WQfX_GiMKN_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Transformer Model and Training params"
      ],
      "metadata": {
        "id": "b7jz61QNNr6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h=8 #Number of self-attention heads\n",
        "d_k=64 #Dimensionality of the linearly projected queries and keys\n",
        "d_v=64 #Dimensionality of the linearly projected values\n",
        "d_model=512 #Dimensionality of the model layer's outputs\n",
        "d_ff=2048 #Dimensionality of the inner FC Layer\n",
        "num_layers=6 #No of layers to repeat for both EncoderLayer and DecoderLayer\n",
        "\n",
        "from transformer_model_params import TransformerModelParams\n",
        "transformerModelParams=TransformerModelParams()\n",
        "transformerModelParams.set_h(h)\\\n",
        "                 .set_d_k(d_k)\\\n",
        "                 .set_d_v(d_v)\\\n",
        "                 .set_d_model(d_model)\\\n",
        "                 .set_d_ff(d_ff)\\\n",
        "                 .set_num_layers(num_layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QDOAYewIKdv",
        "outputId": "0b264df0-6b70-4b12-c397-a17fcad69a0c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformer_model_params.TransformerModelParams at 0x7f3292fb0e20>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=8 #Number of epochs\n",
        "d_k=64 #Dimensionality of the linearly projected queries and keys\n",
        "batch_size=64 #Batch size\n",
        "beta_1=0.9 # beta_1 of ADAM\n",
        "beta_2=0.98 # beta_2 of ADAM\n",
        "epsilon=1e-9 #Epsilon\n",
        "dropout_rate=0.1 #Dropout rate \n",
        "\n",
        "from transformer_training_params import TransformerTrainingParams\n",
        "transformerTrainingParams=TransformerTrainingParams()\n",
        "transformerTrainingParams.set_epochs(epochs)\\\n",
        "                 .set_batch_size(batch_size)\\\n",
        "                 .set_beta_1(beta_1)\\\n",
        "                 .set_beta_2(beta_2)\\\n",
        "                 .set_epsilon(epsilon)\\\n",
        "                 .set_dropout_rate(dropout_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2Dn_OpVNd3q",
        "outputId": "26cd580d-d91c-4c98-e56c-7390db3fccac"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformer_training_params.TransformerTrainingParams at 0x7f32a0040850>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check Word Embedding and  Positional Encoding Layer in Transformers"
      ],
      "metadata": {
        "id": "_l6S6nejvnAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_sequence_length=4\n",
        "vocab_size=5\n",
        "sentences=[[\"I am good\"],[\"She is good\"],[\"All are good\"]]\n",
        "sentence_data=Dataset.from_tensor_slices(sentences)\n",
        "vectorize_layer=TextVectorization(output_sequence_length=output_sequence_length,max_tokens=vocab_size)\n",
        "vectorize_layer.adapt(sentence_data)\n",
        "word_tensors=convert_to_tensor(sentences,dtype=tf.string)\n",
        "vectorized_words=vectorize_layer(word_tensors)\n",
        "print(\"Vocabulary: \",vectorize_layer.get_vocabulary())\n",
        "print(\"Vectorized words : \",vectorized_words)"
      ],
      "metadata": {
        "id": "fUXG_XfWrMkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_length=6\n",
        "word_embedding_layer=Embedding(vocab_size,output_length)\n",
        "embedded_words=word_embedding_layer(vectorized_words)\n",
        "print(\"embedded words are \",embedded_words)"
      ],
      "metadata": {
        "id": "SB4TaTSyvzye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "position_embedding_layer=Embedding(output_sequence_length,output_length)\n",
        "position_indices=tf.range(output_sequence_length)\n",
        "embedded_position_indices=position_embedding_layer(position_indices)\n",
        "print(\"embedded_position_indices are \",embedded_position_indices)"
      ],
      "metadata": {
        "id": "WhehVbzcx5xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_output_embedding=embedded_words+embedded_position_indices\n",
        "print(\"final_output_embedding \",final_output_embedding)"
      ],
      "metadata": {
        "id": "FSw2WDJvygK_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}