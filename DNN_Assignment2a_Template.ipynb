{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maverick98/CDS/blob/main/DNN_Assignment2a_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bda93ba",
      "metadata": {
        "id": "9bda93ba"
      },
      "source": [
        "## Group No 189\n",
        "\n",
        "## Group Member Names:\n",
        "• MAHESH NARAM (2023aa05876): 100%\n",
        "\n",
        "• GIRIJA SHANKAR SAHOO (023AA05235): 100%\n",
        "\n",
        "• SOURAJEET SAHOO (023aa05029): 100%\n",
        "\n",
        "• MANO RANJAN SAHU (2023aa05738): 100%\n",
        "## Journal used for the implementation\n",
        "Journal title: Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations  \n",
        "Authors: Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli  \n",
        "Journal Name: NeurIPS  \n",
        "Year: 2020\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq jiwer\n",
        "!pip install -qq datasets\n"
      ],
      "metadata": {
        "id": "APeaLZJTYx7T"
      },
      "id": "APeaLZJTYx7T",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "6uqFL3UMakmS"
      },
      "id": "6uqFL3UMakmS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0c7f7028",
      "metadata": {
        "id": "0c7f7028"
      },
      "source": [
        "# 1. Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1a096390",
      "metadata": {
        "id": "1a096390"
      },
      "outputs": [],
      "source": [
        "##---------Type the code below this line------------------##\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "import tarfile\n",
        "import requests\n",
        "from jiwer import wer\n",
        "import evaluate\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cc8e0cb",
      "metadata": {
        "id": "3cc8e0cb"
      },
      "source": [
        "# 2. Data Acquisition\n",
        "\n",
        "For the problem identified by you, students have to find the data source themselves from any data source.\n",
        "\n",
        "Provide the URL of the data used.\n",
        "\n",
        "Write Code for converting the above downloaded data into a form suitable for DL\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4b51d895",
      "metadata": {
        "id": "4b51d895"
      },
      "outputs": [],
      "source": [
        "##---------Type the code below this line------------------##\n",
        "\n",
        "\n",
        "url = \"https://www.openslr.org/resources/12/dev-clean.tar.gz\"\n",
        "tar_path = \"dev-clean.tar.gz\"\n",
        "audio_dir = \"LibriSpeech/dev-clean\"\n",
        "\n",
        "# Download the tar.gz file\n",
        "if not os.path.exists(tar_path):\n",
        "    print(\"Downloading dataset...\")\n",
        "    response = requests.get(url)\n",
        "    with open(tar_path, \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "# Extract the tar.gz file\n",
        "if not os.path.exists(audio_dir):\n",
        "    print(\"Extracting dataset...\")\n",
        "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "        tar.extractall()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "102e0e36",
      "metadata": {
        "id": "102e0e36"
      },
      "source": [
        "# 3. Data Preparation\n",
        "\n",
        "Perform the data prepracessing that is required for the data that you have downloaded.\n",
        "\n",
        "\n",
        "This stage depends on the dataset that is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "dd3118eb",
      "metadata": {
        "id": "dd3118eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04a0d5e4-2ad3-4079-e073-8b821d43e92d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2703 audio files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "##---------Type the code below this line------------------##\n",
        "\n",
        "# Let's list all audio files in the dataset directory (dev-clean)\n",
        "audio_files = []\n",
        "for root, dirs, files in os.walk(audio_dir):\n",
        "    for file in files:\n",
        "        if file.endswith(\".flac\"):\n",
        "            audio_files.append(os.path.join(root, file))\n",
        "\n",
        "print(f\"Found {len(audio_files)} audio files.\")\n",
        "\n",
        "# For this example, we'll use only one file to demonstrate the workflow\n",
        "audio_file = audio_files[0]\n",
        "\n",
        "# Load audio using librosa at 16kHz (Wav2Vec2 model works at 16kHz)\n",
        "data, sampling_rate = librosa.load(audio_file, sr=16000)\n",
        "\n",
        "# Step 4: Preprocess audio data using Wav2Vec2 processor\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "input_values = processor(data, return_tensors=\"pt\", sampling_rate=16000).input_values\n",
        "\n",
        "# Splitting into train/test sets for demonstration purposes\n",
        "train_data, test_data = input_values[:, :int(0.8*input_values.size(-1))], input_values[:, int(0.8*input_values.size(-1)):]\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "b310ceab",
      "metadata": {
        "id": "b310ceab"
      },
      "source": [
        "Report the feature representation that is being used for training the model.\n",
        "\n",
        "##---------Type below this line------------------##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ae0b5d2",
      "metadata": {
        "id": "3ae0b5d2"
      },
      "source": [
        "## 4. Deep Neural Network Architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "186bf4d7",
      "metadata": {
        "id": "186bf4d7"
      },
      "source": [
        "## 4.1 Design the architecture that you will be using\n",
        "\n",
        "* CNN / RNN / Transformer as per the journal referenced\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "868d7b27",
      "metadata": {
        "id": "868d7b27"
      },
      "outputs": [],
      "source": [
        "##---------Type the code below this line------------------##\n",
        "class Wav2Vec2ForSpeechRecognition(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Wav2Vec2ForSpeechRecognition, self).__init__()\n",
        "        # Load Wav2Vec2 model for feature extraction\n",
        "        self.wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base\")\n",
        "        # Linear layer for final classification (assuming character-level classification)\n",
        "        self.classifier = nn.Linear(self.wav2vec.config.hidden_size, 29)  # Adjust 29 if needed\n",
        "\n",
        "    def forward(self, input_values):\n",
        "        # Extract features from Wav2Vec2\n",
        "        features = self.wav2vec(input_values).last_hidden_state\n",
        "        # Pass through classifier\n",
        "        logits = self.classifier(features)\n",
        "        return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "575f9e37",
      "metadata": {
        "id": "575f9e37"
      },
      "source": [
        "## 4.2 DNN Report\n",
        "\n",
        "Report the following and provide justification for the same.\n",
        "\n",
        "* Number of layers\n",
        "* Number of units in each layer\n",
        "* Total number of trainable parameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "f5cd8f8f",
      "metadata": {
        "id": "f5cd8f8f"
      },
      "source": [
        "##---------Type the answer below this line------------------##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdbc82a1",
      "metadata": {
        "id": "bdbc82a1"
      },
      "source": [
        "# 5. Training the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "a85e9754",
      "metadata": {
        "id": "a85e9754",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88886ca7-763e-40ef-c3d7-85b277955f07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of Wav2Vec2Model were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: -1.7351213693618774\n",
            "Epoch 2, Loss: -1.6524814367294312\n",
            "Epoch 3, Loss: -1.4827373027801514\n",
            "Epoch 4, Loss: -1.4014805555343628\n",
            "Epoch 5, Loss: -1.223551630973816\n",
            "Epoch 6, Loss: -1.020250678062439\n",
            "Epoch 7, Loss: -0.8831086158752441\n",
            "Epoch 8, Loss: -0.7134530544281006\n",
            "Epoch 9, Loss: -0.5902512073516846\n",
            "Epoch 10, Loss: -0.4782578647136688\n"
          ]
        }
      ],
      "source": [
        "# Configure the training, by using appropriate optimizers, regularizations and loss functions\n",
        "##---------Type the code below this line------------------##\n",
        "# Model initialization\n",
        "model = Wav2Vec2ForSpeechRecognition()\n",
        "\n",
        "# Optimizer and loss function\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
        "loss_function = nn.CTCLoss()  # CTC loss for speech recognition\n",
        "\n",
        "# Simplified training loop (you can expand this for more epochs and larger data)\n",
        "epochs = 10  # Set epochs as needed\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    logits = model(train_data)\n",
        "\n",
        "    # Dummy target for demo purposes (replace with real transcriptions)\n",
        "    target = torch.randint(0, 29, (train_data.size(0), int(0.5*logits.size(1))), dtype=torch.long)  # Random dummy target\n",
        "    input_lengths = torch.full((train_data.size(0),), logits.size(1), dtype=torch.long)\n",
        "    target_lengths = torch.full((train_data.size(0),), target.size(1), dtype=torch.long)\n",
        "\n",
        "    # Calculate CTC Loss\n",
        "    loss = loss_function(logits.permute(1, 0, 2), target, input_lengths, target_lengths)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06f1173c",
      "metadata": {
        "id": "06f1173c"
      },
      "source": [
        "# 6. Test the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "7042235d",
      "metadata": {
        "id": "7042235d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03b1108c-7e24-4893-d654-276187ddffec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of predictions: tensor([[15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "         15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
            "         15, 15, 15, 15]])\n",
            "Shape of target: tensor([[20,  6, 26, 13,  4, 12, 18, 17,  5, 12,  3, 16, 21, 28, 21,  5, 11, 26,\n",
            "          2, 15,  4, 22, 26, 27, 27, 13,  9,  2, 14,  7,  5, 11, 28, 18, 13, 16,\n",
            "         19,  6, 21,  6,  3, 12,  7, 14, 17, 13, 28, 11, 14,  9, 19, 16, 10, 23,\n",
            "          7,  7, 26,  0, 28, 10,  0,  4,  8, 25, 10, 27, 19, 12, 11, 25,  1,  5,\n",
            "         12, 20,  4, 28, 12, 24, 10, 22,  7]])\n"
          ]
        }
      ],
      "source": [
        "##---------Type the code below this line------------------##\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(test_data)\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    print(f\"Shape of predictions: {predictions}\")\n",
        "    print(f\"Shape of target: {target}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb57940c",
      "metadata": {
        "id": "eb57940c"
      },
      "source": [
        "# 7. Report the result\n",
        "\n",
        "1. Plot the training and validation accuracy history.\n",
        "2. Plot the training and validation loss history.\n",
        "3. Report the testing accuracy and loss.\n",
        "4. Show Confusion Matrix for testing dataset.\n",
        "5. Report values for preformance study metrics like accuracy, precision, recall, F1 Score.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "cf409d22",
      "metadata": {
        "id": "cf409d22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d9bb73-e682-4ace-c62f-b516f90afbee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Error Rate (WER): 100.00%\n"
          ]
        }
      ],
      "source": [
        "##---------Type the code below this line------------------##\n",
        "def greedy_decode(logits, processor):\n",
        "    \"\"\"Greedy decoding for converting logits to text.\"\"\"\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "    pred_transcriptions = processor.batch_decode(pred_ids)\n",
        "    return pred_transcriptions\n",
        "\n",
        "# Decode the predictions\n",
        "decoded_preds = greedy_decode(logits, processor)\n",
        "\n",
        "# For demonstration purposes, we'll assume dummy targets (replace these with real transcriptions from the dataset)\n",
        "decoded_target = processor.batch_decode(target)\n",
        "\n",
        "# Step 9: Calculate Word Error Rate (WER)\n",
        "wer_score = wer(decoded_target, decoded_preds)\n",
        "print(f\"Word Error Rate (WER): {wer_score * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RcDDQlfbZQ7E",
      "metadata": {
        "id": "RcDDQlfbZQ7E"
      },
      "source": [
        "### NOTE\n",
        "\n",
        "\n",
        "All Late Submissions will incur a <b>penalty of -2 marks </b>. So submit your assignments on time.\n",
        "\n",
        "Good Luck"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}