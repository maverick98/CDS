{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maverick98/CDS/blob/main/Mano_M5_AST_04_PySpark_Transform_and_Actions_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps9llghv8jX1"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Assignment 4: PySpark Transform and Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeP1PAXf8jYD"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkwaW3k58jYG"
      },
      "source": [
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* Perform RDD (Resilient Distributed Datasets) operations including:\n",
        "        \n",
        "  1.   Transformations\n",
        "  2.   Actions\n",
        "\n",
        "* Obtain an overview of shuffle operations\n",
        "* Implement RDD based model \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3oZtbys8jYL"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nR2ZLd-8jYM"
      },
      "source": [
        "**Overview about Spark, PySpark and Apache Spark in simple language**\n",
        "\n",
        "**Spark:** A data computational framework that handles Big data.\n",
        "\n",
        "**PySpark:** A tool to support Python with Spark\t\n",
        "\n",
        "**Apache Spark:** It is an open-source cluster-computing framework, built around speed, ease of use, and streaming analytics.\n",
        "\n",
        "* Like Spark, PySpark helps data scientists to work with (RDDs) Resilient Distributed Datasets. It is also used to work on Data frames. PySpark can be used to work with machine learning algorithms as well.\n",
        "\n",
        "### ***Spark RDD is a major concept in Apache Spark***\n",
        "\n",
        "**Resilient Distributed Datasets:**\n",
        "\n",
        "**Resilient:**    because RDDs are immutable (can’t be modified once created)                        and fault tolerant.\n",
        "\n",
        "**Distributed:**  because it is distributed across clusters\n",
        "\n",
        "**Dataset:**      because it holds data.\n",
        "\n",
        "**Why RDD?**\n",
        "\n",
        "* Apache Spark lets you treat your input files almost like any other variable, which you cannot do in Hadoop MapReduce. \n",
        "* RDDs are automatically distributed across the network by means of Partitions.\n",
        "\n",
        "RDDs are divided into smaller chunks called Partitions, and when you execute some action, a task is launched per partition. This means, the more the number of partitions, the more will be the parallelism. \n",
        "\n",
        "Spark automatically decides the number of partitions that an RDD has to be divided into, but you can also specify the number of partitions when creating an RDD. These partitions of an RDD are distributed across all the nodes in the network.\n",
        "\n",
        "**Difference between Dataframe and RDD (Resilient Distributed Datasets):**\n",
        "\n",
        "**Dataframe:**\n",
        "* Automatically finds out the schema of the dataset.\n",
        "* Performs aggregation faster than RDDs, as it provides an easy API to perform aggregation operations.\n",
        "\n",
        "**RDD:**\n",
        "* We need to define the schema manually.\n",
        "* RDD is slower than Dataframes to perform simple operations like grouping the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ch8j3bJIiBM"
      },
      "source": [
        "**Creating an RDD**\n",
        "\n",
        "**There are three ways to create an RDD in Spark:**\n",
        "1. Parallelizing already existing collection in the driver program.\n",
        " \n",
        "  The key point to note in a parallelized collection is the number of partitions the dataset is divided into. Spark will run one task for each partition of the cluster. We require two to four partitions for each CPU in the cluster. Spark sets the number of partition based on our cluster. \n",
        "\n",
        "2. Referencing a dataset in an external storage system (e.g. HDFS, Hbase, shared file system).\n",
        "  \n",
        "  In Spark, the distributed dataset can be formed from any data source supported by Hadoop, including the local file system, HDFS, Cassandra, HBase etc. In this, the data is loaded from the external dataset.\n",
        "\n",
        "  * csv (String path): It loads a CSV file and returns the result as a Dataset.\n",
        "\n",
        "  * json (String path): It loads a JSON file (one object per line) and returns the result as a Dataset\n",
        "\n",
        "  * textFile (String path) It loads text files and returns a Dataset of String.\n",
        "\n",
        "3. Creating RDD from already existing RDDs.\n",
        "\n",
        "  Transformation mutates one RDD into another RDD, this transformation is the way to create an RDD from an already existing RDD. This creates a difference between Apache Spark and Hadoop MapReduce. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC1vQgU_JF3p"
      },
      "source": [
        "**Actions/Transformations**\n",
        "\n",
        "There are two types of operations that you can perform on an RDD- \n",
        "* Transformations \n",
        "* Actions. \n",
        "\n",
        "**Transformation** applies some function on an RDD and creates a new RDD, it does not modify the RDD that you apply the function on. Also, the new RDD keeps a pointer to its parent RDD.\n",
        "\n",
        "When you call a transformation, Spark does not execute it immediately, instead it creates a lineage. A lineage keeps track of what all transformations have to be applied on that RDD, including from where it has to read the data.\n",
        "\n",
        "\n",
        "**Action** is used to either save the result to some location or to display it. You can also print the RDD lineage information by using the command: \n",
        "\n",
        "\"filtered.toDebugString\" -> (*filtered* is the RDD here)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ja7Mz2TMhIR"
      },
      "source": [
        "![img](https://cdn.iisc.talentsprint.com/CDS/Images/Pyspark_RDD.JPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YzfoPvJDiTX"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2202500\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjoZJWGErxGf"
      },
      "source": [
        "#@title Please enter your password (your registered phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"9740319263\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBPPuGmBlDIN",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3a5725a0-89e7-40e0-96dc-bcd6bad186b1"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M5_AST_04_PySpark_Transform_and_Actions_A\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "#  ipython.magic(\"sx pip3 install torch\")  \n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Datasets/Spark_Text.txt\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/CDS/Datasets/google_books.csv\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer():\n",
        "  try:\n",
        "    if not Answer:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer\n",
        "  except NameError:\n",
        "    print (\"Please answer Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2202500&recordId=7340\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q9PCT0kBVB3"
      },
      "source": [
        " **Install PySpark**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xzay908G5qQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc742eba-c7bb-4dd6-f912-e07f457b2f64"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=4e88da808068d509304ff584d65410415f6da7aeacdecbb7939f7697018001a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/dc/11/ec201cd671da62fa9c5cc77078235e40722170ceba231d7598\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BobYAePRT6Ok"
      },
      "source": [
        "**Creating Spark Session**\n",
        "\n",
        "Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0 (Instead of having various contexts, everything is encapsulated in a Spark session)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-tj32cQHmBb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "581441f4-1746-4483-d030-3a44d3e351bb"
      },
      "source": [
        "# Start spark session\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf  # User Defined Functions\n",
        "from pyspark.sql.types import StringType\n",
        "spark = SparkSession.builder.appName('Rdd').getOrCreate()\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f7f93da9cd0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4fec52ba7111:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Rdd</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypfIQnQczPMy"
      },
      "source": [
        "# Accessing sparkContext from sparkSession instance.\n",
        "sc = spark.sparkContext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kt4YX3C3dGT"
      },
      "source": [
        "### Spark Python Transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuBu_zaiYWX-"
      },
      "source": [
        "**map()** - A map transformation is useful when we need to transform an RDD by applying a function to each element. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vJJS0NS_o8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420f2c81-5f40-40df-d064-e607c1edb179"
      },
      "source": [
        "# Return a new RDD by applying a function to each element of this RDD.\n",
        "rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
        "sorted(rdd.map(lambda x: (x, 1)).collect())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 1), ('b', 1), ('c', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize([113,21,333])\n",
        "sorted(rdd.map(lambda x: (x,1)).collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31CWIU2lQhWY",
        "outputId": "30f0cda9-a15e-4f09-802c-2086df21dcf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(21, 1), (113, 1), (333, 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRmlweKeDvjf"
      },
      "source": [
        "**take()** - Take the first num elements of the RDD.\n",
        "\n",
        "It works by first scanning one partition, and use the results from that partition to estimate the number of additional partitions needed to satisfy the limit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGskOAX2_s0x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec60ae8-01c8-4924-cef3-8a2dd6115c8f"
      },
      "source": [
        "sc.parallelize([2, 3, 4, 5, 6]).cache().take(20) #take()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 3, 4, 5, 6]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.parallelize([1,2,3,4,5]).take(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoZYZIXLSP2x",
        "outputId": "406e3d0b-fec2-4b6d-9223-b7896972766b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.parallelize(range(500),500).filter(lambda x: x>456).take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS2HyeuZSW63",
        "outputId": "e0e1d939-9607-4728-a046-67fef23e7b50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[457, 458, 459, 460, 461]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oZr8ukWkHAW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8359130c-c991-4e63-a5cd-c4480e8d4184"
      },
      "source": [
        "sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(30) #take()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[91, 92, 93, 94, 95, 96, 97, 98, 99]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3fEehJeEguG"
      },
      "source": [
        "**flatMap()** - The flatMap transformation will return a new RDD by first applying a function to all elements of this RDD, and then flattening the results. This is the main difference between the flatMap and *map transformations.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s0=sc.parallelize([3,4,5])\n",
        "s0.map(lambda x:[x,x*x]).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6aCR-ntTZYd",
        "outputId": "e6d5da7c-08a5-4558-93bb-4771e272380e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 9], [4, 16], [5, 25]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s0=sc.parallelize([3,4,5])\n",
        "s0.flatMap(lambda x:[x,x*x]).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWn5yYK6TOHo",
        "outputId": "3b5ce81f-dbfd-43db-9ea9-33540096d266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 9, 4, 16, 5, 25]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAelIgUTf3Sy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73bc260c-b48a-40b4-bd68-2052c5277a57"
      },
      "source": [
        "# YOUR CODE HERE to create s0 with [3,4,5]\n",
        "s0=sc.parallelize([3,4,5])\n",
        "s0.flatMap(lambda x: [x, x*x]).collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 9, 4, 16, 5, 25]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJLRChBQxnmG"
      },
      "source": [
        "Compare the same function using map()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBJegE2cxmAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb56d656-b14d-4398-892e-a37396622c82"
      },
      "source": [
        "sc.parallelize([3,4,5]).map(lambda x: [x,  x*x]).collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 9], [4, 16], [5, 25]]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WihTRM55aQzK"
      },
      "source": [
        "**filter()** - The filter transformation returns a new dataset formed by selecting  those elements of the source on which func returns true."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd=sc.parallelize([1, 2, 3, 4, 5])\n",
        "rdd.filter(lambda x:x%2 ==0).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qot1DPsSTybH",
        "outputId": "49142ea7-eb6e-47d9-b610-d849ad4289e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgypZMSQGyR_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c64102-8ebc-4d63-ce74-f936419a227d"
      },
      "source": [
        "# YOUR CODE HERE to create rdd with [1, 2, 3, 4, 5]\n",
        "rdd.filter(lambda x: x % 2 == 0).collect() # Return a new RDD containing only the elements that satisfy a predicate."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FV3bYIKJi31"
      },
      "source": [
        "**groupByKey()** - We can apply the “groupByKey” transformations on (key,val) pair RDD. The “groupByKey” will group the values for each key in the original RDD. It will create a new pair, where the original key corresponds to this collected group of values."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "x.groupByKey().collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkZtDgSiU5bA",
        "outputId": "5494c78a-a34e-4d92-cf6e-b7eceeee0db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('b', <pyspark.resultiterable.ResultIterable at 0x7f7f94d0de50>),\n",
              " ('a', <pyspark.resultiterable.ResultIterable at 0x7f7f94d0d1c0>)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "x.groupByKey().map(lambda x:  (x[0], list(x[1]))  ).collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mtRMfwIUDb-",
        "outputId": "7adb5413-aa60-4671-9a2f-012e5d09a554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('b', [1]), ('a', [1, 1])]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyh3Z6mDOMbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa273eb-aa04-49b5-9779-0e057a6c4b49"
      },
      "source": [
        "# YOUR CODE HERE to create x with [(\"a\", 1), (\"b\", 1), (\"a\", 1)]\n",
        "x.groupByKey().map(lambda x : (x[0], list(x[1]))).collect() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('b', [1]), ('a', [1, 1])]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjbYWjBCLfUn"
      },
      "source": [
        "**reduceByKey()** - Merge the values for each key using an associative reduce function."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import add\n",
        "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "sorted(rdd.reduceByKey(lambda x,y: x+y).collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXE9HMfYVKrn",
        "outputId": "1817e457-f503-4e05-9800-78f4e5da934d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 2), ('b', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_kSwLv6OmzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15d6c7b-f5fe-402c-f49c-2347a8c1dd58"
      },
      "source": [
        "from operator import add\n",
        "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "sorted(rdd.reduceByKey(add).collect())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('a', 2), ('b', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV-_pmJVLfl5"
      },
      "source": [
        "**mapPartitions()** - Is similar to map, but runs separately on each partition (block) of the RDD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2UtmrsS5uA0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "545a7e1a-066c-4846-9a12-d9662ddf8d30"
      },
      "source": [
        "wordsList = ['cat', 'elephant', 'rat', 'rat', 'cat','dog']\n",
        "\n",
        "wordsRDD = sc.parallelize(wordsList, 3) # number of partitions - 4\n",
        "\n",
        "print(wordsRDD.collect())\n",
        "\n",
        "itemsRDD = wordsRDD.mapPartitions(lambda iterator: [','.join(iterator)])\n",
        "# mapPartitions() loops through 4 partitions and combines('rat,cat') in 4th iteration.\n",
        "# YOUR CODE HERE to print all elements of itemsRDD\n",
        "itemsRDD.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat', 'elephant', 'rat', 'rat', 'cat', 'dog']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cat,elephant', 'rat,rat', 'cat,dog']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBZnXOHiHMjU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52c02cbc-26d5-49e6-ee50-4f05444f44e7"
      },
      "source": [
        "L = range(1,10)\n",
        "\n",
        "# YOUR CODE HERE to create 'parallel' with L         # number of partitions - 3\n",
        "parallel=sc.parallelize(L,3)\n",
        "\n",
        "def f1(iterator): \n",
        "   return sum(iterator)\n",
        "\n",
        "def f(iterator): \n",
        "  yield sum(iterator)\n",
        "\n",
        "parallel.mapPartitions(f).collect()\n",
        "\n",
        "# Results [6,15,24] are created because mapPartitions() loops through 3 partitions, \n",
        "# Partion 1: 1+2+3 = 6, Partition 2: 4+5+6 = 15, Partition 3: 7+8+9 = 24"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 15, 24]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rclESZfdRbMM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3c3e25-972d-43f9-9e00-2606561c56c7"
      },
      "source": [
        "# YOUR CODE HERE to create 'rdd' with [1, 2, 3, 4]      # number of partitions - 2\n",
        "rdd=sc.parallelize([1,2,3,4])\n",
        "def f(iterator):\n",
        "  yield sum(iterator)\n",
        "\n",
        "rdd.mapPartitions(f).collect() \n",
        "\n",
        "# Results [3, 7], partition 1 : 1+2 = 3, partition 2 : 3+4 =7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 7]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-SUV2VcJjMJ"
      },
      "source": [
        "**mapPartitionsWithIndex()** - Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dgV6f21Us_m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dfa397e-d787-468f-fa31-21f5f1b0e478"
      },
      "source": [
        "rdd = sc.parallelize([1, 2, 3, 4,5,6,7], 4)\n",
        "def f(splitIndex, iterator): yield splitIndex\n",
        "rdd.mapPartitionsWithIndex(f).sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82fgwxs8kzy5"
      },
      "source": [
        "### Spark Python Actions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaYgbjhklcT0"
      },
      "source": [
        "**Creating an RDD to explain \"RDD actions with Examples\"**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seJ35vrGleKN"
      },
      "source": [
        "data=[(\"Z\", 1),(\"A\", 20),(\"B\", 30),(\"C\", 40),(\"B\", 30),(\"B\", 60)]\n",
        "\n",
        "inputRDD = spark.sparkContext.parallelize(data)\n",
        "  \n",
        "# YOUR CODE HERE to create listRdd with [1,2,3,4,5,3,2]\n",
        "listRdd = spark.sparkContext.parallelize([1,2,3,4,5,3,2])\n",
        "from operator import add"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTRyVfftnLaD"
      },
      "source": [
        "After creating two RDD’s as given above, we use these two as and when necessary to demonstrate the RDD actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjcr2K9fk0LT"
      },
      "source": [
        "**first()** – Return the first element in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR-hsPBqnds7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5df2a41f-b644-481c-f075-cf4df584169e"
      },
      "source": [
        "#first\n",
        "print(\"first :  \"+str(listRdd.first()))\n",
        "\n",
        "# YOUR CODE HERE to print the first element of inputRDD \n",
        "print(\"first :  \"+str(inputRDD.first()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "first :  1\n",
            "first :  ('Z', 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFIF21pnk0tc"
      },
      "source": [
        "**take()** – Return the first num elements of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8iyLrZ0nopK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a2b179-9e00-4089-99a2-a134f2fb22c9"
      },
      "source": [
        "#take()\n",
        "print(\"take : \"+str(listRdd.take(2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "take : [1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5dG25M-k02C"
      },
      "source": [
        "**takeSample()** – Return the subset of the dataset in an Array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J58_ENwkn15A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8c06ddb-b03d-453a-e413-43ceb6dabc9b"
      },
      "source": [
        "print(\"take : \"+str(listRdd.takeSample(0,3))) # ([1,2,3,4,5,3,2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "take : [2, 4, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XddOZ8LRk08Y"
      },
      "source": [
        "**takeOrdered()** – Return the first num (smallest) elements from the dataset and this is the opposite of the take() action. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nptmLS_qoSKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ed3a6a-d759-446f-ba4c-307423059db7"
      },
      "source": [
        "print(\"takeOrdered : \"+ str(listRdd.takeOrdered(2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "takeOrdered : [1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-chsPGWPk1EQ"
      },
      "source": [
        "**collect()** - Return the complete dataset as an Array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMDnTnA0pNHG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c98db7-b6e0-42b6-aa9c-890e4c32426d"
      },
      "source": [
        "#Collect\n",
        "data = listRdd.collect()\n",
        "# YOUR CODE HERE to display data\n",
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 3, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1How8QYk1NK"
      },
      "source": [
        "**count()** – Return the count of elements in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5YNtF_RpW6y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c231085c-e713-4a9c-e44f-76c0f2596ff8"
      },
      "source": [
        "print(\"Count : \"+str(listRdd.count()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count : 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNSjv_LCpcAU"
      },
      "source": [
        "**countByValue()** – Return Map[T,Long] key representing each unique value in dataset and value represents count each value present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij3GMKfbpcLs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3bdc349-ddbb-49ea-ae5a-ad1e9d065bca"
      },
      "source": [
        "print(\"countByValue :  \"+str(listRdd.countByValue()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "countByValue :  defaultdict(<class 'int'>, {1: 1, 2: 2, 3: 2, 4: 1, 5: 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNHqoneIplsk"
      },
      "source": [
        "**reduce()** – Reduces the elements of the dataset using the specified binary operator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McRegvjapl0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12013834-79e8-4710-c3c8-a737fc31ea42"
      },
      "source": [
        "redRes=listRdd.reduce(add)\n",
        "# YOUR CODE HERE to display redRes \n",
        "redRes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yklH98ELqV_q"
      },
      "source": [
        "**top()** – Return top n elements from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pj1f2tsCqWWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d056a649-1542-4e49-8d53-71b9ec5e46a2"
      },
      "source": [
        "print(\"top : \"+str(listRdd.top(2)))\n",
        "# YOUR CODE HERE to display top 2 elements from inputRDD"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top : [5, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OONqU-CTtEkZ"
      },
      "source": [
        "**fold()** - Aggregate the elements of each partition, and then the results for all the partitions, using a given associative function and a neutral \"zero value.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euxrIYOYtFEu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b8d309-0ab3-4351-cf2a-87ba73a6b494"
      },
      "source": [
        "foldRes=listRdd.fold(0, add)\n",
        "# YOUR CODE HERE to display foldRes\n",
        "print(foldRes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khV8HhGc0lhx"
      },
      "source": [
        "**foldByKey()** -  is quite similar to fold() both use a zero value of the same type of the data in our RDD and combination function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aGQdKuM0L9n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e576cde8-ab58-4979-fdae-c0d7c336aed3"
      },
      "source": [
        "inputRDD.foldByKey(0, add).collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('C', 40), ('Z', 1), ('A', 20), ('B', 120)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehIGu2bG0-kc"
      },
      "source": [
        "**reduceByKey()** - Merge the values for each key using an associative reduce function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PXKBSyi0-zi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d877ec3-2187-4b5a-acdf-5e6acac7c6a5"
      },
      "source": [
        "sorted(inputRDD.reduceByKey(add).collect())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('A', 20), ('B', 120), ('C', 40), ('Z', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6S5yGzv13Ck"
      },
      "source": [
        "**combineByKey()** - Generic function to combine the elements for each key using a custom set of aggregation functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSpVQHCL13Ls",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1a6db9f-22cc-445c-aded-a94a87b2c0ee"
      },
      "source": [
        "def f(inputRDD):\n",
        "  # YOUR CODE HERE to return inputRDD\n",
        "  return inputRDD\n",
        "def add(A, B):\n",
        "  return A + str(B)\n",
        "sorted(inputRDD.combineByKey(str, add, add).collect())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('A', '20'), ('B', '303060'), ('C', '40'), ('Z', '1')]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFXAbayxXKtw"
      },
      "source": [
        "### PySpark User Defined Functions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXEXD6LUXFu7"
      },
      "source": [
        "* PySpark UDF is a User Defined Function that is used to create a reusable \n",
        "function in Spark.\n",
        "\n",
        "* Once UDF is created, that can be re-used on multiple DataFrames and SQL (after registering).\n",
        "\n",
        "* The default type of the udf() is StringType."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCyKMMvCGRkk"
      },
      "source": [
        "Created dataframe with two columns \"Seqno\" and \"Name\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvCmOPMTDtZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e097c7e4-a240-4e32-f705-7b83732bdb63"
      },
      "source": [
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "# YOUR CODE HERE to create a dataframe and show\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "\n",
        "df.show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+\n",
            "|Seqno|Name        |\n",
            "+-----+------------+\n",
            "|1    |john jones  |\n",
            "|2    |tracey smith|\n",
            "|3    |amy sanders |\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lon2qElWGW8Z"
      },
      "source": [
        "Applying UDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9TQBD5JZTh3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e52b3852-97c8-452f-d8b2-fd423048a579"
      },
      "source": [
        "# creating a udf using lambda\n",
        "convertUDF = udf(lambda z: z.upper())\n",
        "\n",
        "df.select(col(\"Seqno\"), convertUDF(col(\"Name\")).alias(\"Name\") ).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------+\n",
            "|Seqno|Name        |\n",
            "+-----+------------+\n",
            "|1    |JOHN JONES  |\n",
            "|2    |TRACEY SMITH|\n",
            "|3    |AMY SANDERS |\n",
            "+-----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiEIyKJ4xmKj"
      },
      "source": [
        "#### **Shuffle Operations**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XP9I8tYbl_i"
      },
      "source": [
        "Shuffling is a mechanism PySpark uses to redistribute the data across different executors and even across machines. PySpark shuffling triggers when we perform certain transformation operations like gropByKey(), reduceByKey(), join() on RDDS\n",
        "\n",
        "Spark also supports transformations with wide dependencies, such as groupByKey and reduceByKey. In these dependencies, the data required to compute the records in a single partition can reside in many partitions of the parent dataset.\n",
        "\n",
        "To perform these transformations, all of the tuples with the same key must end up in the same partition, processed by the same task. To satisfy this requirement, Spark performs a shuffle, which transfers data around the cluster and results in a new stage with a new set of partitions. \n",
        "\n",
        "For example, consider the following code:\n",
        "\n",
        "**sc.textFile(\"someFile.txt\").map(mapFunc).flatMap(flatMapFunc).filter(filterFunc).count()**\n",
        "\n",
        "It runs a single action, count, which depends on a sequence of three transformations on a dataset derived from a text file. This code runs in a single stage because none of the outputs of these three transformations depend on data that comes from different partitions than their inputs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huYvlG3IAq_A"
      },
      "source": [
        "**Below is an example implementing RDD based model to count the words given in a file**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEuTe7NTfvSO"
      },
      "source": [
        "\n",
        "\n",
        "To implement RDD based model, we have used the text file (**Spark_Text.txt**) which includes Apache Spark notes/information. This text file contains 5 paragraphs of information on Spark.\n",
        "\n",
        "We would perform RDD Transformations and Actions on the file to count the words given in the text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1WbnApmkBle"
      },
      "source": [
        "rdd = sc.textFile(\"Spark_Text.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gpTlIzzYH3V"
      },
      "source": [
        "#To lower the case of each word of a document, we can use the map transformation.\n",
        "\n",
        "def Func(lines):\n",
        "      lines = lines.lower()\n",
        "      lines = lines.split()\n",
        "      return lines\n",
        "# YOUR CODE HERE to apply Func on rdd using map and create rdd1 \n",
        "rdd1 = rdd.map(Func)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-h8E_vJiplH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49aaa456-be13-4192-d34f-b3448d065337"
      },
      "source": [
        "rdd1.take(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['apache',\n",
              "  'spark',\n",
              "  'lets',\n",
              "  'you',\n",
              "  'treat',\n",
              "  'your',\n",
              "  'input',\n",
              "  'files',\n",
              "  'almost',\n",
              "  'like',\n",
              "  'any',\n",
              "  'other',\n",
              "  'variable,',\n",
              "  'which',\n",
              "  'you',\n",
              "  'cannot',\n",
              "  'do',\n",
              "  'in',\n",
              "  'hadoop',\n",
              "  'mapreduce.']]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3zknNLjYH-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472dcb0b-8b20-425a-c47c-4283fb948328"
      },
      "source": [
        "#To get the flat output, we need to apply a transformation which will flatten the output, The transformation “flatMap\" will help here:\n",
        "rdd2 = rdd.flatMap(Func)\n",
        "# YOUR CODE HERE to display first three elements of rdd2\n",
        "rdd2 = rdd.flatMap(Func)\n",
        "rdd2.take(3)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apache', 'spark', 'lets']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0wfA1urYIB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc01acc-62b5-413b-8005-e765d472e181"
      },
      "source": [
        "rdd3 = rdd2.filter(lambda x:x!= '')\n",
        "rdd3.take(7)  # We can check first 7 elements of “rdd3” by applying take action."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['apache', 'spark', 'lets', 'you', 'treat', 'your', 'input']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c9lxazfYIFM"
      },
      "source": [
        "rdd3_mapped = rdd3.map(lambda x: (x,1))\n",
        "# YOUR CODE HERE to apply groupByKey method to rdd3_mapped and create 'rdd3_grouped'\n",
        "rdd3_grouped = rdd3_mapped.groupByKey() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIozl8hjYV0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef76d463-a9bb-40d4-83fc-3141fa1dd2ae"
      },
      "source": [
        "rdd3_mapped.reduceByKey(lambda x,y: x+y).map(lambda x:(x[1],x[0])).sortByKey(False).take(200)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(13, 'the'),\n",
              " (7, 'a'),\n",
              " (6, 'of'),\n",
              " (6, 'you'),\n",
              " (5, 'rdd'),\n",
              " (4, 'that'),\n",
              " (4, 'to'),\n",
              " (4, 'it'),\n",
              " (3, 'are'),\n",
              " (3, 'when'),\n",
              " (3, 'an'),\n",
              " (3, 'spark'),\n",
              " (3, 'rdds'),\n",
              " (3, 'number'),\n",
              " (3, 'be'),\n",
              " (3, 'partitions'),\n",
              " (3, 'has'),\n",
              " (2, 'in'),\n",
              " (2, 'partitions,'),\n",
              " (2, 'execute'),\n",
              " (2, 'is'),\n",
              " (2, 'more'),\n",
              " (2, 'rdd.'),\n",
              " (2, 'new'),\n",
              " (2, 'rdd,'),\n",
              " (2, 'keeps'),\n",
              " (2, 'which'),\n",
              " (2, 'automatically'),\n",
              " (2, 'distributed'),\n",
              " (2, 'across'),\n",
              " (2, 'divided'),\n",
              " (2, 'and'),\n",
              " (2, 'some'),\n",
              " (2, 'all'),\n",
              " (2, 'function'),\n",
              " (2, 'on'),\n",
              " (2, 'creates'),\n",
              " (2, 'does'),\n",
              " (2, 'not'),\n",
              " (1, 'treat'),\n",
              " (1, 'input'),\n",
              " (1, 'files'),\n",
              " (1, 'like'),\n",
              " (1, 'other'),\n",
              " (1, 'variable,'),\n",
              " (1, 'cannot'),\n",
              " (1, 'do'),\n",
              " (1, 'hadoop'),\n",
              " (1, 'network'),\n",
              " (1, 'means'),\n",
              " (1, 'into'),\n",
              " (1, 'task'),\n",
              " (1, 'means,'),\n",
              " (1, 'decides'),\n",
              " (1, 'but'),\n",
              " (1, 'creating'),\n",
              " (1, 'these'),\n",
              " (1, 'nodes'),\n",
              " (1, 'network.'),\n",
              " (1, 'applies'),\n",
              " (1, 'on.(remember'),\n",
              " (1, 'also,'),\n",
              " (1, 'pointer'),\n",
              " (1, 'parent'),\n",
              " (1, 'call'),\n",
              " (1, 'transformation,'),\n",
              " (1, 'instead'),\n",
              " (1, 'lineage.'),\n",
              " (1, 'transformations'),\n",
              " (1, 'applied'),\n",
              " (1, 'where'),\n",
              " (1, 'read'),\n",
              " (1, 'apache'),\n",
              " (1, 'lets'),\n",
              " (1, 'your'),\n",
              " (1, 'almost'),\n",
              " (1, 'any'),\n",
              " (1, 'mapreduce.'),\n",
              " (1, 'by'),\n",
              " (1, 'partitions.'),\n",
              " (1, 'smaller'),\n",
              " (1, 'chunks'),\n",
              " (1, 'called'),\n",
              " (1, 'action,'),\n",
              " (1, 'launched'),\n",
              " (1, 'per'),\n",
              " (1, 'partition.'),\n",
              " (1, 'will'),\n",
              " (1, 'parallelism.'),\n",
              " (1, 'into,'),\n",
              " (1, 'can'),\n",
              " (1, 'also'),\n",
              " (1, 'specify'),\n",
              " (1, 'transformation'),\n",
              " (1, 'modify'),\n",
              " (1, 'apply'),\n",
              " (1, 'resilient/immutable).'),\n",
              " (1, \"it's\"),\n",
              " (1, 'immediately,'),\n",
              " (1, 'lineage'),\n",
              " (1, 'track'),\n",
              " (1, 'what'),\n",
              " (1, 'including'),\n",
              " (1, 'from'),\n",
              " (1, 'data')]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTbilp60aHcl"
      },
      "source": [
        "**In the below example we can see Spark Transformations in Python using a CSV file.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_9aAnEafyOc"
      },
      "source": [
        "We will use this CSV file (**Google_Books.csv**) to work on Spark Transformations.\n",
        "\n",
        "This data was acquired from the Google Books store. Google API was used to acquire the data. Nine features were gathered for each book in the data set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P357wQsFb5XS"
      },
      "source": [
        "book_names = sc.textFile(\"google_books.csv\")\n",
        "rows = book_names.map(lambda line: line.split(\",\")) #we are creating a new RDD called “rows” by splitting every row in the book_names RDD.  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0wNy70CdRpC"
      },
      "source": [
        "for row in rows.take(rows.count()):\n",
        "  # YOUR CODE HERE to display first element of row\n",
        "  print(row[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n01Rlq6wee8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a415f82-fcfc-4302-96e8-af1f93e0c2b9"
      },
      "source": [
        "for row in rows.take(10):\n",
        "    print(row[1])\n",
        "  # YOUR CODE HERE to print first element of row"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "authors\n",
            "['Wendelin Van Draanen']\n",
            "\n",
            "\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n",
            "['Jessica Keyes']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvdfQzEXjDq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d1ba44e-44ea-4366-d583-5f59772519c8"
      },
      "source": [
        "# filter() - Creating a new RDD by returning only the elements that satisfy the search filter.\n",
        "rows.filter(lambda line: \"Inward Journey\" in line).collect() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Inward Journey',\n",
              "  '',\n",
              "  'en',\n",
              "  \"['Medical']\",\n",
              "  '',\n",
              "  'NOT_MATURE',\n",
              "  'Open Court Publishing Company',\n",
              "  '1983-01',\n",
              "  '133']]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKguXKc1lAtF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26933e8a-b984-4e30-f213-c16205201a6f"
      },
      "source": [
        "# groupByKey() The following groups all titles to their publisher. Operates on value pairs\n",
        "rows = book_names.map(lambda line: line.split(\",\"))\n",
        "titleToPublisher = rows.map(lambda n: (str(n[0]),str(n[6]) )).groupByKey()\n",
        "titleToPublisher.map(lambda x : {x[0]: list(x[1])}).take(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': ['publisher']},\n",
              " {'The Boston Directory ...': ['']},\n",
              " {\"The CIO's Guide to Oracle Products and Solutions\": ['CRC Press']},\n",
              " {'Implementing the IT Balanced Scorecard': ['CRC Press',\n",
              "   'CRC Press',\n",
              "   'CRC Press']},\n",
              " {'Social Software Engineering': ['CRC Press']}]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgSwVENIPcM6"
      },
      "source": [
        "# @title Select the False Statement: { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Answer = \"flatMap function always produces a single value as output for each input value\" #@param [\"\", \"Map transforms an RDD of length N into another RDD of length N\",\"flatMap function always produces a single value as output for each input value\",\"The reduce operation shuffles and reduces the output obtained from the map\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good, But Not Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"Good\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"No\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Didn't use\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzAZHt1zw-Y-",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7e74405-6288-4384-d309-6e6fb547ac4c"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 7340\n",
            "Date of submission:  27 Jan 2023\n",
            "Time of submission:  15:32:27\n",
            "View your submissions: https://cds.iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ]
    }
  ]
}