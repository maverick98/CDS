{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7rc1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maverick98/CDS/blob/main/M6_SNB_MiniProject_1_Market_Basket_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "numerical-princess"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "\n",
        "##  A program by IISc and TalentSprint\n",
        "\n",
        "### Mini Project 1: Market Basket analysis"
      ],
      "id": "numerical-princess"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-kxaHhwXEp9"
      },
      "source": [
        "**DISCLAIMER:** THIS NOTEBOOK IS PROVIDED ONLY AS A REFERENCE SOLUTION NOTEBOOK FOR THE MINI-PROJECT. THERE MAY BE OTHER POSSIBLE APPROACHES/METHODS TO ACHIEVE THE SAME RESULTS."
      ],
      "id": "n-kxaHhwXEp9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "manufactured-poison"
      },
      "source": [
        "## Learning Objectives"
      ],
      "id": "manufactured-poison"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "strong-yellow"
      },
      "source": [
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* extract summary level insight from a given dataset\n",
        "\n",
        "* Integrate the data and identify the underlying pattern or structure\n",
        "\n",
        "* understand the fundamentals of market basket analysis\n",
        "\n",
        "* construct \"rules\" that provide concrete recommendations for businesses"
      ],
      "id": "strong-yellow"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "downtown-indie"
      },
      "source": [
        "## Dataset"
      ],
      "id": "downtown-indie"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "short-spank"
      },
      "source": [
        "The dataset chosen for this mini project is **Instacart Dataset**. The dataset is anonymized and contains a sample of over 3 million grocery orders from more than 200,000 Instacart users. For each user, there are orders between 4 and 100, with the sequence of products purchased in each order. The dataset also includes the products in each order, the time of day and day of week of each order, the name and aisle/department of each product, which are stored across various files."
      ],
      "id": "short-spank"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "secure-cabin"
      },
      "source": [
        "## Problem Statement"
      ],
      "id": "secure-cabin"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "running-shade"
      },
      "source": [
        "\n",
        "Extract association rules and find groups of frequently purchased items from a large-scale grocery orders dataset."
      ],
      "id": "running-shade"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndQNKsjS7c04"
      },
      "source": [
        "## Grading = 10 Points"
      ],
      "id": "ndQNKsjS7c04"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyric-fairy"
      },
      "source": [
        "#### Import required packages"
      ],
      "id": "lyric-fairy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blond-klein"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import scipy\n",
        "import os\n",
        "from mlxtend.frequent_patterns import apriori\n",
        "from mlxtend.frequent_patterns import association_rules"
      ],
      "id": "blond-klein",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abandoned-workstation"
      },
      "source": [
        "## **Stage 1**: Data Wrangling"
      ],
      "id": "abandoned-workstation"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adjusted-excess"
      },
      "source": [
        "We have five different files:\n",
        "\n",
        "    - orders.csv\n",
        "    - order_products__train.csv\n",
        "    - products.csv\n",
        "    - aisles.csv\n",
        "    - departments.csv\n",
        "\t\n",
        "These files contain the neccesary data to solve the problem. Load all the files correctly, after observing the header level details, data records etc\n",
        "\t\n",
        "**Hint:** Use `read_csv` from pandas"
      ],
      "id": "adjusted-excess"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "valued-discount",
        "cellView": "form"
      },
      "source": [
        "#@title Download the data\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/Datasets/Instacart.zip\n",
        "!unzip -qq Instacart.zip"
      ],
      "id": "valued-discount",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thrown-handle"
      },
      "source": [
        "### Load the data\n",
        "\n",
        "Load all the given datasets"
      ],
      "id": "thrown-handle"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "occupational-reverse"
      },
      "source": [
        "root = '/content/Instacart/'\n",
        "orders = pd.read_csv(root + 'orders.csv')\n",
        "order_products_train = pd.read_csv(root + 'order_products__train.csv')\n",
        "products = pd.read_csv(root + 'products.csv')\n",
        "aisles = pd.read_csv(root + 'aisles.csv')\n",
        "departments = pd.read_csv(root + 'departments.csv')"
      ],
      "id": "occupational-reverse",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adjustable-criterion"
      },
      "source": [
        "or"
      ],
      "id": "adjustable-criterion"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "australian-module"
      },
      "source": [
        "datasets = {}\n",
        "\n",
        "for i in os.listdir('/content/Instacart/'):\n",
        "  print(i)\n",
        "  datasets[i] = pd.read_csv(\"Instacart/\"+i) \n",
        "    \n",
        "datasets = dict(sorted(datasets.items()))\n",
        "datasets.keys()"
      ],
      "id": "australian-module",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ancient-prize"
      },
      "source": [
        "names  = list(datasets.keys())\n",
        "names"
      ],
      "id": "ancient-prize",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vocational-declaration"
      },
      "source": [
        "### Data Integration (1 point)"
      ],
      "id": "vocational-declaration"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sporting-fraction"
      },
      "source": [
        "As the required data is present in different files, we need to integrate all the five to make single dataframe/dataset. For that purpose, use the unique identifier provided in all the dataframes so that it can be used to map the data in different files correctly.\n",
        "\n",
        "**Example:** `product_id` is available in both `products` dataframe and `order_products__train` dataframe, we can merge these two into a single dataframe based on `product_id`\n",
        "\n",
        "**Hint:** [pd.merge](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html)"
      ],
      "id": "sporting-fraction"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fabulous-thumbnail"
      },
      "source": [
        "df1 = datasets[names[2]]\n",
        "df1.columns, df1.shape"
      ],
      "id": "fabulous-thumbnail",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "central-matrix"
      },
      "source": [
        "df2 = df1.merge(datasets[names[-1]], on='product_id')\n",
        "df2.columns, df2.shape"
      ],
      "id": "central-matrix",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "white-chess"
      },
      "source": [
        "df3 = df2.merge(datasets[names[0]], on='aisle_id')\n",
        "df3.columns, df3.shape"
      ],
      "id": "white-chess",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ranking-essence"
      },
      "source": [
        "df4 = df3.merge(datasets[names[3]], on='order_id')\n",
        "df4.columns, df4.shape"
      ],
      "id": "ranking-essence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "destroyed-disclosure"
      },
      "source": [
        "df5 = df4.merge(datasets[names[1]], on='department_id')\n",
        "df5.columns, df5.shape"
      ],
      "id": "destroyed-disclosure",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "digital-refund"
      },
      "source": [
        "df5.head()"
      ],
      "id": "digital-refund",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "artificial-valuable"
      },
      "source": [
        "final_df = df5"
      ],
      "id": "artificial-valuable",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsZ15M0Vkm-Q"
      },
      "source": [
        "### Understanding relationships and new insights from the data (3 points)"
      ],
      "id": "DsZ15M0Vkm-Q"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faced-tomato"
      },
      "source": [
        "1.  How many times was each product ordered?\n",
        "\n",
        "    **Hint:** group orders by product\n",
        "    \n",
        "\n",
        "2.  Find the number of orders per department and visualize using an appropriate plot\n",
        "\n",
        "\n",
        "3.  On which day of the week do customers tend to buy more groceries? Which are the peak hours\n",
        "of shopping? \n",
        "\n",
        "  * Find the frequency of orders on week days using an appropriate plot \n",
        "  * Find the frequency of orders during hours of the day using an appropriate plot?\n",
        "  \n",
        "\n",
        "4. Find the ratio of Re-ordered and Not Re-ordered products and visualize it\n",
        "\n",
        "5. Plot the heatmap of Re-order ratio of the Day of week vs Hour of day"
      ],
      "id": "faced-tomato"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alien-alliance"
      },
      "source": [
        "### Group orders by products and get how many times each product was ordered"
      ],
      "id": "alien-alliance"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usual-boards"
      },
      "source": [
        "g = final_df.product_id.value_counts()\n",
        "g = pd.DataFrame(g)\n",
        "g.reset_index(inplace=True)\n",
        "g.columns = [\"product_id\",\"count\"]\n",
        "g_products = g.merge(datasets['products.csv'],on=\"product_id\")\n",
        "g_products.head()"
      ],
      "id": "usual-boards",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "automated-bankruptcy"
      },
      "source": [
        "plt.figure(figsize=(50, 40))\n",
        "g_products.head(20).plot(kind=\"bar\",x=\"product_name\",y=\"count\")\n",
        "plt.show()"
      ],
      "id": "automated-bankruptcy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "significant-breakfast"
      },
      "source": [
        "(Banana is the top ordered product)"
      ],
      "id": "significant-breakfast"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "graphic-scenario"
      },
      "source": [
        "### Find the number of orders per department\n",
        "\n",
        "Hint: Groupby"
      ],
      "id": "graphic-scenario"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "impressive-ratio"
      },
      "source": [
        "g = final_df.department_id.value_counts()\n",
        "g = pd.DataFrame(g)\n",
        "g.reset_index(inplace=True)\n",
        "g.columns = [\"department_id\",\"count\"]\n",
        "g_dept = g.merge(datasets['departments.csv'],on=\"department_id\")\n",
        "g_dept.head(5)"
      ],
      "id": "impressive-ratio",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olympic-promise"
      },
      "source": [
        "g_dept.plot(kind=\"bar\",x=\"department\",y=\"count\")\n",
        "plt.show()"
      ],
      "id": "olympic-promise",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hindu-ethernet"
      },
      "source": [
        "### Find the frequency of orders on week days"
      ],
      "id": "hindu-ethernet"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adjusted-comment"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.countplot(x=\"order_dow\", data=final_df)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xlabel('Day of week', fontsize=12)\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.title(\"Frequency of order by week day\", fontsize=15)\n",
        "plt.show()"
      ],
      "id": "adjusted-comment",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "julian-immune"
      },
      "source": [
        "### Find the frequency of orders for hours of the day"
      ],
      "id": "julian-immune"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "collectible-bottle"
      },
      "source": [
        "plt.figure(figsize=(12,8))\n",
        "sns.countplot(x=\"order_hour_of_day\", data=final_df)\n",
        "plt.ylabel('Count', fontsize=12)\n",
        "plt.xlabel('Hour of day', fontsize=12)\n",
        "plt.xticks(rotation='vertical')\n",
        "plt.title(\"Frequency of order by hour of day\", fontsize=15)\n",
        "plt.show()"
      ],
      "id": "collectible-bottle",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "identical-celebration"
      },
      "source": [
        "### Find the ratio of Re-ordered and Not Re-ordered products and visualize"
      ],
      "id": "identical-celebration"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPO19uOKUZJr"
      },
      "source": [
        "final_df[final_df['reordered']==1]['product_id'].max() #nunique()"
      ],
      "id": "kPO19uOKUZJr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "personal-superintendent"
      },
      "source": [
        "sns.distplot(final_df[final_df['reordered']==1]['product_id'],  kde=False, label='Reordered')\n",
        "sns.distplot(final_df[final_df['reordered']==0]['product_id'],  kde=False, label='Not reordered')\n",
        "\n",
        "plt.legend(prop={'size': 12})\n",
        "plt.show()"
      ],
      "id": "personal-superintendent",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boring-dutch"
      },
      "source": [
        "### Plot the heatmap of Re-order ratio of Day of week vs Hour of day ?"
      ],
      "id": "boring-dutch"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fewer-chassis"
      },
      "source": [
        "grouped_df = final_df.groupby([\"order_dow\", \"order_hour_of_day\"])[\"reordered\"].aggregate(\"mean\").reset_index()\n",
        "grouped_df = grouped_df.pivot('order_dow', 'order_hour_of_day', 'reordered')\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(grouped_df)\n",
        "plt.title(\"Reorder ratio of Day of week Vs Hour of day\")\n",
        "plt.show()"
      ],
      "id": "fewer-chassis",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dangerous-rendering"
      },
      "source": [
        "## **Stage 2:** Create a basket (4 points)"
      ],
      "id": "dangerous-rendering"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "italian-guinea"
      },
      "source": [
        "As the dataset contains huge amount of data, let us take a subset of the data to extract the association rules from it. \n",
        "\n",
        "**Assumption:** Segment the data by considering the 100 most frequent ordered items. Please note it is just an assumption. You can consider 'n' frequent order items as per your choice.\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "- Drop the unwanted columns\n",
        "\n",
        "- Find the frequencies of orders based on the products and  consider the 100 most frequent order items.\n",
        "\n",
        "    **Hint:** Count the frequencies of orders for each product_id using `groupby()` and `count()` respectively\n",
        "\n",
        "- Extract the records of 100 most frequent items (which are extracted in previous step) from combined dataframe.\n",
        "\n",
        "- Create a Pivot table with `order_id` as index and `product_name` as columns and `reorder` as values. \n",
        "\n",
        "    - set the `order_id` as index using set_index()\n",
        "    - fill all the nan values with 0\n",
        "\n",
        "- After performing the above step, there are a lot of zeros in the data, make sure that any positive values are converted to a 1 and anything less than 0 is set to 0.\n"
      ],
      "id": "italian-guinea"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "worthy-portuguese"
      },
      "source": [
        "product_counts = final_df.groupby('product_id')['order_id'].count().reset_index().rename(columns = {'order_id':'frequency'})\n",
        "product_counts = product_counts.sort_values('frequency', ascending=False)[0:100].reset_index(drop = True)\n",
        "product_counts.head(10)"
      ],
      "id": "worthy-portuguese",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "white-empty"
      },
      "source": [
        "freq_products = list(product_counts.product_id)\n",
        "freq_products[1:10]"
      ],
      "id": "white-empty",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "native-gateway"
      },
      "source": [
        "order_products = final_df[final_df.product_id.isin(freq_products)]\n",
        "order_products.shape"
      ],
      "id": "native-gateway",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "artistic-sight"
      },
      "source": [
        "# basket = order_products.groupby(['order_id', 'product_name'])['reordered'].count().unstack().reset_index().fillna(0).set_index('order_id')\n",
        "# basket.head()"
      ],
      "id": "artistic-sight",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "underlying-rental"
      },
      "source": [
        "or"
      ],
      "id": "underlying-rental"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faced-candy"
      },
      "source": [
        "basket = order_products.pivot_table(columns='product_name', values='reordered',index='order_id' ).reset_index().fillna(0).set_index('order_id')"
      ],
      "id": "faced-candy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tribal-ecology"
      },
      "source": [
        "def encode_units(x):\n",
        "    if x <= 0:\n",
        "        return 0\n",
        "    if x >= 1:\n",
        "        return 1 \n",
        "    \n",
        "basket = basket.applymap(encode_units)\n",
        "basket.head()"
      ],
      "id": "tribal-ecology",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "presidential-fleece"
      },
      "source": [
        "## **Stage 3:** Apply Apriori algorithm (2 points)"
      ],
      "id": "presidential-fleece"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "portable-dutch"
      },
      "source": [
        "- As the dataset contains huge amount of data, let us take a subset of the data to extract the association rules from it.\n",
        "\n",
        "  **Assumption:** Segment the basket by considering 100000 records. Please note its just an assumption, you can consider 'n'  records as per your choice.\n",
        "\n",
        "  **Hint:** [apriori](http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.frequent_patterns/)\n",
        "\n",
        "- Find the association rules and make a dataframe"
      ],
      "id": "portable-dutch"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "activated-blade"
      },
      "source": [
        "shortbasket = basket[:100000]"
      ],
      "id": "activated-blade",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unique-retrieval"
      },
      "source": [
        "frequent_items = apriori(shortbasket, min_support=0.01, use_colnames=True)\n",
        "frequent_items.head()"
      ],
      "id": "unique-retrieval",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "progressive-manual"
      },
      "source": [
        "rules = association_rules(frequent_items, metric=\"lift\", min_threshold=1)\n",
        "rules.sort_values('lift', ascending=False)"
      ],
      "id": "progressive-manual",
      "execution_count": null,
      "outputs": []
    }
  ]
}